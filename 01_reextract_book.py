#!/usr/bin/env python3
"""
–ü–µ—Ä–µ—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä PDF —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
"""

import fitz
import os
import json
import re
from pathlib import Path
from tqdm import tqdm
import hashlib


class ProperPDFExtractor:
    """–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä PDF —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã"""
    
    def __init__(self, pdf_path, output_dir="extracted_fixed"):
        self.pdf_path = pdf_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.metadata = {
            "total_pages": 0,
            "chapters": [],
            "extraction_complete": False,
            "book_title": "",
            "book_info": {}
        }
        
    def extract_all(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–∑ PDF —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π"""
        print("üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º PDF –¥–æ–∫—É–º–µ–Ω—Ç...")
        doc = fitz.open(self.pdf_path)
        self.metadata["total_pages"] = len(doc)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
        self._extract_book_metadata(doc)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
        toc = doc.get_toc()
        
        all_pages = []
        print("üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
        for page_num in tqdm(range(len(doc)), desc="–ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü"):
            page = doc[page_num]
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            text = page.get_text()
            all_pages.append({
                'page_num': page_num,
                'text': text
            })
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥–ª–∞–≤
        if toc and len(toc) > 0:
            print("üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–ª–∞–≤...")
            chapters_data = self._smart_split_by_toc(all_pages, toc, doc)
        else:
            print("üìä –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥–ª–∞–≤—ã –ø–æ 30 —Å—Ç—Ä–∞–Ω–∏—Ü...")
            chapters_data = self._split_by_pages(all_pages, 30)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—ã
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏...")
        chapter_number = 0
        for chapter_data in tqdm(chapters_data, desc="–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–∞–≤"):
            self._save_chapter(
                chapter_data["title"], 
                chapter_data["pages"], 
                chapter_number,
                chapter_data.get("start_page", 0),
                chapter_data.get("end_page", 0)
            )
            
            self.metadata["chapters"].append({
                "number": chapter_number,
                "title": chapter_data["title"],
                "start_page": chapter_data.get("start_page", 0),
                "end_page": chapter_data.get("end_page", 0),
                "page_count": chapter_data.get("end_page", 0) - chapter_data.get("start_page", 0) + 1,
                "status": "extracted"
            })
            
            chapter_number += 1
        
        doc.close()
        
        self.metadata["extraction_complete"] = True
        self._save_metadata()
        
        print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   üìë –ì–ª–∞–≤: {len(self.metadata['chapters'])}")
        print(f"   üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {self.metadata['total_pages']}")
        
        return self.metadata
    
    def _smart_split_by_toc(self, all_pages, toc, doc):
        """–£–º–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –≥–ª–∞–≤—ã –ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é"""
        chapters = []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º TOC - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤—ã –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –≤–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        filtered_toc = []
        
        for i, entry in enumerate(toc):
            level, title, page_num = entry
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF –≤ –∏–Ω–¥–µ–∫—Å Python (PDF –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 1)
            page_index = page_num - 1
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —É—Ä–æ–≤–Ω—è 0-2 –¥–ª—è –±–æ–ª—å—à–µ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
            if level <= 2 or any(keyword in title.lower() for keyword in 
                                ['chapter', 'part', 'introduction', '–≥–ª–∞–≤–∞', '—á–∞—Å—Ç—å', 'appendix', 
                                 'process area', 'section']):
                filtered_toc.append((level, title, page_index))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ–ª–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã
        MIN_PAGES = 2  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥–ª–∞–≤—ã
        merged_toc = []
        
        i = 0
        while i < len(filtered_toc):
            current_level, current_title, current_page = filtered_toc[i]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤—ã
            if i + 1 < len(filtered_toc):
                next_page = filtered_toc[i + 1][2]
            else:
                next_page = len(all_pages) - 1
            
            # –ï—Å–ª–∏ –≥–ª–∞–≤–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è, –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å–ª–µ–¥—É—é—â–∏–º–∏
            accumulated_title = current_title
            accumulated_start = current_page
            accumulated_end = next_page
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –≥–ª–∞–≤—ã
            while accumulated_end - accumulated_start < MIN_PAGES and i + 1 < len(filtered_toc):
                i += 1
                if i < len(filtered_toc):
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω–µ—Ü
                    if i + 1 < len(filtered_toc):
                        accumulated_end = filtered_toc[i + 1][2]
                    else:
                        accumulated_end = len(all_pages) - 1
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ —Ä–∞–∑–¥–µ–ª —Ç–æ–≥–æ –∂–µ —É—Ä–æ–≤–Ω—è, –æ–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                    if filtered_toc[i][0] <= current_level + 1:
                        if len(accumulated_title) < 100:
                            accumulated_title += f" / {filtered_toc[i][1]}"
            
            merged_toc.append((current_level, accumulated_title, accumulated_start, accumulated_end))
            i += 1
        
        # –°–æ–∑–¥–∞–µ–º –≥–ª–∞–≤—ã
        for i, (level, title, start_page, end_page) in enumerate(merged_toc):
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π
            if i > 0 and start_page <= merged_toc[i-1][3]:
                start_page = merged_toc[i-1][3] + 1
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≥–ª–∞–≤—ã
            chapter_pages = all_pages[start_page:end_page+1]
            
            chapters.append({
                "title": title,
                "pages": chapter_pages,
                "start_page": start_page,
                "end_page": end_page
            })
        
        return chapters
    
    def _split_by_pages(self, all_pages, pages_per_chapter):
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –≥–ª–∞–≤—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–∞–Ω–∏—Ü"""
        chapters = []
        
        for i in range(0, len(all_pages), pages_per_chapter):
            chapter_pages = all_pages[i:i + pages_per_chapter]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            title = f"–ß–∞—Å—Ç—å {i // pages_per_chapter + 1}"
            
            chapters.append({
                "title": title,
                "pages": chapter_pages,
                "start_page": i,
                "end_page": min(i + pages_per_chapter - 1, len(all_pages) - 1)
            })
        
        return chapters
    
    def _save_chapter(self, title, pages, chapter_num, start_page, end_page):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–∞–≤—ã –≤ JSON —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã"""
        filename = f"chapter_{chapter_num:03d}.json"
        filepath = self.output_dir / filename
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        full_text = '\n'.join([p['text'] for p in pages])
        
        # –ù–ï –æ—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        # –¢–æ–ª—å–∫–æ —É–±–∏—Ä–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        full_text = re.sub(r'\n{4,}', '\n\n\n', full_text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        paragraphs = self._split_into_proper_paragraphs(full_text)
        
        chapter_data = {
            "number": chapter_num,
            "title": title,
            "start_page": start_page,
            "end_page": end_page,
            "paragraphs": paragraphs,
            "word_count": len(full_text.split())
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(chapter_data, f, ensure_ascii=False, indent=2)
    
    def _split_into_proper_paragraphs(self, text):
        """–£–º–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        paragraphs = []
        current_paragraph = []
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - –≤–æ–∑–º–æ–∂–Ω—ã–π –∫–æ–Ω–µ—Ü –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            if not stripped:
                if current_paragraph:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–µ–∫—É—â–µ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
                    para_text = ' '.join(current_paragraph)
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                    if len(para_text) > 20:  # –ú–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤
                        paragraphs.append(para_text)
                    current_paragraph = []
            else:
                # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞:
                # - –°—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏
                # - –°—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–º–µ—Ä–∞ (–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
                # - –°—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –±—É–ª–ª–∏—Ç–∞
                # - –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç—Ä–æ–∫–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–ª–∞—Å—å —Ç–æ—á–∫–æ–π/–≤–æ–ø—Ä–æ—Å–æ–º/–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ–º
                
                if current_paragraph and self._is_new_paragraph(current_paragraph[-1], stripped):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                    para_text = ' '.join(current_paragraph)
                    if len(para_text) > 20:
                        paragraphs.append(para_text)
                    current_paragraph = [stripped]
                else:
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                    current_paragraph.append(stripped)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        if current_paragraph:
            para_text = ' '.join(current_paragraph)
            if len(para_text) > 20:
                paragraphs.append(para_text)
        
        return paragraphs
    
    def _is_new_paragraph(self, prev_line, curr_line):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –Ω–æ–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ"""
        # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç—Ä–æ–∫–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å —Ç–æ—á–∫–æ–π/–≤–æ–ø—Ä–æ—Å–æ–º/–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ–º
        if prev_line.rstrip().endswith(('.', '!', '?', ':', ';')):
            # –ò —Ç–µ–∫—É—â–∞—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã –∏–ª–∏ —Ü–∏—Ñ—Ä—ã
            if curr_line and (curr_line[0].isupper() or curr_line[0].isdigit()):
                return True
        
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–∞—Ä–∫–µ—Ä–∞ —Å–ø–∏—Å–∫–∞
        list_markers = ['‚Ä¢', '‚óè', '‚óã', '‚ñ†', '‚ñ°', '‚ñ™', '‚ñ´', '-', '*', '‚Äì']
        if any(curr_line.startswith(marker) for marker in list_markers):
            return True
        
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–∞
        if re.match(r'^\d+[.)]\s', curr_line):
            return True
        
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –±—É–∫–≤–µ–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞
        if re.match(r'^[a-zA-Z][.)]\s', curr_line):
            return True
        
        return False
    
    def _extract_book_metadata(self, doc):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏"""
        metadata = doc.metadata
        if metadata:
            self.metadata["book_info"] = {
                "title": metadata.get("title", ""),
                "author": metadata.get("author", ""),
                "subject": metadata.get("subject", ""),
                "keywords": metadata.get("keywords", ""),
            }
    
    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        metadata_path = self.output_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ—ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏ PDF —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π...")
    
    extractor = ProperPDFExtractor("book.pdf")
    metadata = extractor.extract_all()
    
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏:")
    print(f"   –í—Å–µ–≥–æ –≥–ª–∞–≤: {len(metadata['chapters'])}")
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {metadata['total_pages']}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–π –≥–ª–∞–≤—ã
    first_chapter = Path("extracted_proper/chapter_000.json")
    if first_chapter.exists():
        with open(first_chapter, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä –≥–ª–∞–≤—ã '{data['title']}':")
            print(f"   –ü–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(data['paragraphs'])}")
            print(f"   –°–ª–æ–≤: {data['word_count']}")
            if data['paragraphs']:
                print(f"   –ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ: {data['paragraphs'][0][:100]}...")