#!/usr/bin/env python3
"""
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
"""

import json
import re
from pathlib import Path
from collections import Counter
from typing import Dict, List, Set
import argparse

class TermExtractor:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        self.patterns = [
            # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (2+ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤)
            (r'\b[A-Z]{2,}\b', 'abbreviation'),
            
            # CamelCase —Ç–µ—Ä–º–∏–Ω—ã
            (r'\b[A-Z][a-z]+(?:[A-Z][a-z]+)+\b', 'camelcase'),
            
            # –¢–µ—Ä–º–∏–Ω—ã —Å —Ç–æ—á–∫–∞–º–∏ (C.M.M.I.)
            (r'\b[A-Z](?:\.[A-Z])+\.?\b', 'dotted'),
            
            # Level —Å —á–∏—Å–ª–æ–º
            (r'\bLevel\s+\d+\b', 'level'),
            
            # –°–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (Process Area, Generic Goal)
            (r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', 'compound'),
            
            # –í–µ—Ä—Å–∏–∏ (v1.3, Version 1.3)
            (r'\b[Vv](?:ersion)?\s*\d+\.\d+\b', 'version'),
            
            # –û—Ç–¥–µ–ª—å–Ω—ã–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            (r'(?<=[–∞-—è–ê-–Ø\s])[A-Za-z]+(?=[–∞-—è–ê-–Ø\s])', 'english_word'),
        ]
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        self.stop_words = {
            'Figure', 'Table', 'Chapter', 'Section', 'Page',
            'The', 'This', 'That', 'These', 'Those',
            'A', 'An', 'And', 'Or', 'But', 'In', 'On', 'At', 'To', 'For',
            'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X'
        }
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã CMMI –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.known_terms = {
            'CMMI', 'SEI', 'SCAMPI', 'CMU',
            'Process Area', 'Maturity Level', 'Capability Level',
            'Generic Goal', 'Specific Goal', 
            'Generic Practice', 'Specific Practice',
            'Generic Goals', 'Specific Goals',
            'CAR', 'CM', 'DAR', 'IPM', 'MA', 'OPD', 'OPF', 'OPM', 
            'OPP', 'OT', 'PI', 'PMC', 'PP', 'PPQA', 'QPM', 'RD', 
            'REQM', 'RSKM', 'SAM', 'TS', 'VAL', 'VER',
            'Software Engineering Institute',
            'Carnegie Mellon University',
            'Development', 'Acquisition', 'Services',
            'Agile', 'Scrum', 'DevOps', 'Waterfall',
            'High Maturity', 'Appraisal', 'Assessment'
        }
    
    def extract_from_text(self, text: str) -> Dict[str, Set[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        terms_by_type = {}
        
        for pattern, term_type in self.patterns:
            matches = re.findall(pattern, text)
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            filtered = [m for m in matches if m not in self.stop_words]
            if filtered:
                if term_type not in terms_by_type:
                    terms_by_type[term_type] = set()
                terms_by_type[term_type].update(filtered)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
        known_found = set()
        for term in self.known_terms:
            if term in text:
                known_found.add(term)
        
        if known_found:
            terms_by_type['known'] = known_found
        
        return terms_by_type
    
    def extract_from_file(self, file_path: Path) -> Dict[str, Set[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ JSON —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        all_terms = {}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if 'title' in data:
            terms = self.extract_from_text(data['title'])
            for term_type, term_set in terms.items():
                if term_type not in all_terms:
                    all_terms[term_type] = set()
                all_terms[term_type].update(term_set)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if 'paragraphs' in data:
            for para in data['paragraphs']:
                if para and not para.startswith('[IMAGE_'):
                    terms = self.extract_from_text(para)
                    for term_type, term_set in terms.items():
                        if term_type not in all_terms:
                            all_terms[term_type] = set()
                        all_terms[term_type].update(term_set)
        
        return all_terms
    
    def extract_from_directory(self, input_dir: str = "translations"):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
        input_path = Path(input_dir)
        
        if not input_path.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {input_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
        
        all_terms = {}
        term_frequency = Counter()
        
        files = list(input_path.glob("*_translated.json"))
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(files)}")
        
        for file in files:
            file_terms = self.extract_from_file(file)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ—Ä–º–∏–Ω—ã
            for term_type, term_set in file_terms.items():
                if term_type not in all_terms:
                    all_terms[term_type] = set()
                all_terms[term_type].update(term_set)
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
                for term in term_set:
                    term_frequency[term] += 1
        
        return all_terms, term_frequency
    
    def save_terms(self, all_terms: Dict, frequency: Counter, output_file: str = "extracted_terms.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ —Ñ–∞–π–ª"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sets –≤ lists –¥–ª—è JSON
        terms_list = {}
        for term_type, term_set in all_terms.items():
            terms_list[term_type] = sorted(list(term_set))
        
        # –¢–æ–ø —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        top_frequent = dict(frequency.most_common(100))
        
        output = {
            "total_unique_terms": sum(len(terms) for terms in all_terms.values()),
            "terms_by_type": terms_list,
            "term_frequency": top_frequent,
            "statistics": {
                term_type: len(term_set) 
                for term_type, term_set in all_terms.items()
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        return output
    
    def print_statistics(self, all_terms: Dict, frequency: Counter):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º"""
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:")
        print("=" * 60)
        
        total = sum(len(terms) for terms in all_terms.values())
        print(f"üìù –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {total}")
        print()
        
        # –ü–æ —Ç–∏–ø–∞–º
        print("üìÇ –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for term_type, terms in all_terms.items():
            print(f"  ‚Ä¢ {term_type:15} : {len(terms):4} —Ç–µ—Ä–º–∏–Ω–æ–≤")
        
        # –¢–æ–ø-20 —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö
        print("\nüîù –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:")
        for term, count in frequency.most_common(20):
            print(f"  {count:3}x - {term}")
        
        # –ü—Ä–∏–º–µ—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print("\nüìù –ü—Ä–∏–º–µ—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for term_type, terms in all_terms.items():
            examples = sorted(list(terms))[:5]
            print(f"\n  {term_type}:")
            for ex in examples:
                print(f"    ‚Ä¢ {ex}")


def main():
    parser = argparse.ArgumentParser(description='–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –ø–µ—Ä–µ–≤–æ–¥–æ–≤')
    parser.add_argument('--input-dir', default='translations',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏')
    parser.add_argument('--output', default='extracted_terms.json',
                       help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤')
    parser.add_argument('--min-frequency', type=int, default=2,
                       help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–∞')
    
    args = parser.parse_args()
    
    print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –ø–µ—Ä–µ–≤–æ–¥–æ–≤")
    print("=" * 60)
    
    extractor = TermExtractor()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã
    result = extractor.extract_from_directory(args.input_dir)
    
    if result:
        all_terms, frequency = result
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
        if args.min_frequency > 1:
            filtered_terms = {}
            for term_type, terms in all_terms.items():
                filtered = {t for t in terms if frequency[t] >= args.min_frequency}
                if filtered:
                    filtered_terms[term_type] = filtered
            all_terms = filtered_terms
            print(f"\nüîé –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ —á–∞—Å—Ç–æ—Ç–µ >= {args.min_frequency}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_data = extractor.save_terms(all_terms, frequency, args.output)
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {args.output}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        extractor.print_statistics(all_terms, frequency)
        
        print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
        print(f"\nüí° –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
        print(f"   python3 08_generate_phonetics.py --terms {args.output}")
        
        return output_data


if __name__ == "__main__":
    main()