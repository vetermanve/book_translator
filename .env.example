# ===========================
# РЕЖИМ РАБОТЫ / OPERATION MODE
# ===========================

# Использовать локальную модель через Ollama (true/false)
# Use local model via Ollama (true/false)
USE_LOCAL_MODEL=false

# ===========================
# ОБЛАЧНЫЙ API / CLOUD API
# ===========================

# DeepSeek API Key (для облачного режима / for cloud mode)
# Get your API key from https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Optional: API endpoint (default is https://api.deepseek.com)
# DEEPSEEK_API_ENDPOINT=https://api.deepseek.com/v1

# ===========================
# ЛОКАЛЬНАЯ МОДЕЛЬ / LOCAL MODEL
# ===========================

# URL Ollama сервера (для локального режима / for local mode)
OLLAMA_BASE_URL=http://localhost:11434/v1

# Модель Ollama / Ollama model
# Варианты / Options: 
# - deepseek-coder:6.7b-instruct (рекомендуется / recommended)
# - qwen2.5:7b (хорошо для русского / good for Russian)
# - mistral:7b (универсальная / universal)
# - llama3.2:latest (последняя версия / latest)
OLLAMA_MODEL=deepseek-coder:6.7b-instruct

# ===========================
# НАСТРОЙКИ ГЕНЕРАЦИИ / GENERATION SETTINGS
# ===========================

# Максимальное количество параллельных потоков / Max parallel workers
MAX_WORKERS=25

# Максимум токенов на запрос / Max tokens per request
MAX_TOKENS=4000

# Температура генерации / Temperature (0.0-1.0)
# Меньше = более детерминированный / Lower = more deterministic
TEMPERATURE=0.3