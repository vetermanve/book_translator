# üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Ollama

## üìã –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama —Å –º–æ–¥–µ–ª—å—é
./setup_ollama.sh

# 2. –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –≤ .env
echo "USE_LOCAL_MODEL=true" >> .env

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
python3 test_local_model.py

# 4. –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–≤–æ–¥–∞
python3 03_translate_parallel.py --all
```

## üîß –î–µ—Ç–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏:

```bash
./setup_ollama.sh
```

–°–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç:
- ‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∫—É Ollama (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
- ‚úÖ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ Ollama
- ‚úÖ –ó–∞–≥—Ä—É–∑–∫—É –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
- ‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
- ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ .env

### –®–∞–≥ 2: –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏

–ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ `setup_ollama.sh` –≤–∞–º –±—É–¥–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –≤—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å:

1. **deepseek-coder:6.7b-instruct** (4GB) - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–¥–µ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
   - –•–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç CMMI

2. **qwen2.5:7b** (4.4GB) - –•–æ—Ä–æ—à–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
   - –û—Ç–ª–∏—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
   - –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å

3. **mistral:7b** (4.1GB) - –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è
   - –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é
   - –•–æ—Ä–æ—à–∞—è –æ–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

4. **llama3.2:latest** (4.7GB) - –ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è
   - –°–∞–º–∞—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
   - –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤

### –®–∞–≥ 3: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è .env

–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ .env –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã:

```bash
# –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º Ollama
USE_LOCAL_MODEL=true
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=deepseek-coder:6.7b-instruct
```

### –®–∞–≥ 4: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–æ–º

```bash
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
./start_ollama.sh

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
pkill ollama

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
curl http://localhost:11434/api/tags
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã
python3 test_local_model.py

# –¢–µ—Å—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
python3 -c "
from deepseek_translator import DeepSeekTranslator
t = DeepSeekTranslator()
print(t.translate_text('Hello world'))
"
```

## ‚öôÔ∏è –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–∏—Å—Ç–µ–º–µ

- **RAM**: –ú–∏–Ω–∏–º—É–º 8GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16GB)
- **–î–∏—Å–∫**: 5-10GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π
- **CPU**: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ AVX2 –∂–µ–ª–∞—Ç–µ–ª—å–Ω–∞)
- **GPU**: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (—É—Å–∫–æ—Ä–∏—Ç —Ä–∞–±–æ—Ç—É –≤ 5-10 —Ä–∞–∑)

## üéØ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–∞:

| –†–µ–∂–∏–º | –°–∫–æ—Ä–æ—Å—Ç—å | –°—Ç–æ–∏–º–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ |
|-------|----------|-----------|----------|
| –û–±–ª–∞—á–Ω—ã–π API | ~2-3 —Å–µ–∫/–∑–∞–ø—Ä–æ—Å | $0.14/1M —Ç–æ–∫–µ–Ω–æ–≤ | –û—Ç–ª–∏—á–Ω–æ–µ |
| –õ–æ–∫–∞–ª—å–Ω—ã–π CPU | ~10-20 —Å–µ–∫/–∑–∞–ø—Ä–æ—Å | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | –•–æ—Ä–æ—à–µ–µ |
| –õ–æ–∫–∞–ª—å–Ω—ã–π GPU | ~2-5 —Å–µ–∫/–∑–∞–ø—Ä–æ—Å | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | –•–æ—Ä–æ—à–µ–µ |

## üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å NVIDIA GPU:

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ GPU
nvidia-smi

# –ó–∞–ø—É—Å–∫ Ollama —Å GPU
OLLAMA_NUM_GPU=1 ollama serve

# –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
export OLLAMA_NUM_GPU=1
```

–î–ª—è Mac —Å Apple Silicon GPU —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.

## üîç –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –°–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ä—Ç
lsof -i :11434

# –£–±–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –µ—Å–ª–∏ –∑–∞–Ω—è—Ç
kill -9 $(lsof -t -i:11434)

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
./start_ollama.sh
```

### –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Ä—É—á–Ω—É—é
ollama pull deepseek-coder:6.7b-instruct

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
ollama list
```

### –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
ollama pull tinyllama:latest  # 637MB

# –ò–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
OLLAMA_MAX_LOADED_MODELS=1 ollama serve
```

## üìù –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏

```bash
# –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
sed -i '' 's/USE_LOCAL_MODEL=false/USE_LOCAL_MODEL=true/' .env

# –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –æ–±–ª–∞—á–Ω–æ–º—É API
sed -i '' 's/USE_LOCAL_MODEL=true/USE_LOCAL_MODEL=false/' .env
```

## üéì –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

–ú–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:

```bash
# –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
ollama pull aya:8b  # –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å

# –î–ª—è –∫–æ–¥–∞
ollama pull codellama:7b
ollama pull starcoder2:7b

# –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
ollama pull saiga:7b  # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
```

## üìö –†–µ—Å—É—Ä—Å—ã

- [Ollama Documentation](https://github.com/ollama/ollama)
- [–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π](https://ollama.ai/library)
- [DeepSeek Models](https://github.com/deepseek-ai)
- [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏](https://github.com/ollama/ollama/blob/main/docs/faq.md)

---

üí° **–°–æ–≤–µ—Ç**: –î–ª—è –±–æ–ª—å—à–∏—Ö –∫–Ω–∏–≥ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –∏–ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –Ω–æ—á—å—é –Ω–∞ CPU.