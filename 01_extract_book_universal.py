#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è PDF –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤
"""

import fitz
import os
import json
import re
from pathlib import Path
from tqdm import tqdm
import hashlib
from typing import List, Dict, Tuple, Optional
import unicodedata


class UniversalBookExtractor:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è –ª—é–±—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∫–Ω–∏–≥"""
    
    def __init__(self, input_path, output_dir="extracted_fixed"):
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
        self.file_format = self._detect_format()
        
        # –û–±—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.metadata = {
            "total_pages": 0,
            "chapters": [],
            "extraction_complete": False,
            "book_title": "",
            "book_info": {},
            "source_format": self.file_format,
            "extraction_method": "",
            "statistics": {}
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏
        self.settings = {
            "min_paragraph_length": 10,
            "sentences_per_paragraph": 8,  # –î–ª—è —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            "chars_per_chapter": 10000,     # –î–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É
            "pages_per_chapter": 30,         # –î–ª—è PDF –±–µ–∑ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
            "smart_paragraph_split": True,   # –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            "preserve_formatting": True,     # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
            "extract_images": True,          # –ò–∑–≤–ª–µ–∫–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            "clean_ocr_artifacts": True      # –û—á–∏—â–∞—Ç—å OCR –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        }
    
    def _detect_format(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞"""
        suffix = self.input_path.suffix.lower()
        if suffix == '.pdf':
            return 'pdf'
        elif suffix in ['.txt', '.text']:
            return 'txt'
        elif suffix == '.md':
            return 'markdown'
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            try:
                with open(self.input_path, 'r', encoding='utf-8') as f:
                    f.read(1000)
                return 'txt'
            except:
                return 'pdf'
    
    def extract_all(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        print(f"üìö –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {self.input_path.name}")
        print(f"üìÇ –§–æ—Ä–º–∞—Ç: {self.file_format.upper()}")
        
        if self.file_format == 'pdf':
            return self._extract_from_pdf()
        else:
            return self._extract_from_text()
    
    def _extract_from_pdf(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ PDF —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""
        print("üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º PDF –¥–æ–∫—É–º–µ–Ω—Ç...")
        
        try:
            doc = fitz.open(str(self.input_path))
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è PDF: {e}")
            return None
        
        self.metadata["total_pages"] = len(doc)
        self.metadata["extraction_method"] = "pdf_advanced"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self._extract_pdf_metadata(doc)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ
        all_pages = []
        print(f"üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ {len(doc)} —Å—Ç—Ä–∞–Ω–∏—Ü...")
        
        for page_num in tqdm(range(len(doc)), desc="–ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü"):
            page = doc[page_num]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            text = self._extract_page_text(page)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            if self.settings["extract_images"]:
                images = self._extract_page_images(page, page_num)
            
            all_pages.append({
                'page_num': page_num,
                'text': text,
                'images': images
            })
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥–ª–∞–≤
        chapters_data = self._determine_chapter_structure(doc, all_pages)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—ã
        self._save_chapters(chapters_data)
        
        doc.close()
        
        self._finalize_extraction()
        return self.metadata
    
    def _extract_from_text(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —É–º–Ω—ã–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º"""
        print("üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª...")
        
        try:
            with open(self.input_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
        except UnicodeDecodeError:
            # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            for encoding in ['latin-1', 'cp1251', 'cp1252']:
                try:
                    with open(self.input_path, 'r', encoding=encoding) as f:
                        full_text = f.read()
                    print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞: {encoding}")
                    break
                except:
                    continue
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª")
                return None
        
        self.metadata["extraction_method"] = "text_smart"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞
        text_stats = self._analyze_text_structure(full_text)
        self.metadata["statistics"] = text_stats
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞:")
        print(f"   ‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {text_stats['char_count']:,}")
        print(f"   ‚Ä¢ –°–ª–æ–≤: {text_stats['word_count']:,}")
        print(f"   ‚Ä¢ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {text_stats['sentence_count']:,}")
        print(f"   ‚Ä¢ –ü–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {text_stats['paragraph_count']:,}")
        print(f"   ‚Ä¢ –ü–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫: {text_stats['newline_count']:,}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–ª–∞–≤—ã
        chapters_data = self._detect_text_chapters(full_text, text_stats)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—ã
        self._save_chapters(chapters_data)
        
        self._finalize_extraction()
        return self.metadata
    
    def _extract_pdf_metadata(self, doc):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö PDF"""
        metadata = doc.metadata
        if metadata:
            self.metadata["book_info"] = {
                "title": metadata.get("title", ""),
                "author": metadata.get("author", ""),
                "subject": metadata.get("subject", ""),
                "keywords": metadata.get("keywords", ""),
                "creator": metadata.get("creator", ""),
                "producer": metadata.get("producer", ""),
                "creation_date": str(metadata.get("creationDate", "")),
                "modification_date": str(metadata.get("modDate", ""))
            }
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ
            if metadata.get("title"):
                self.metadata["book_title"] = metadata.get("title")
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                self.metadata["book_title"] = self.input_path.stem
    
    def _extract_page_text(self, page) -> str:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ú–µ—Ç–æ–¥ 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        text = page.get_text()
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã
        if len(text.strip()) < 100:
            # –ú–µ—Ç–æ–¥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–ª–æ–∫–∞–º–∏
            blocks = page.get_text("blocks")
            text = "\n".join([b[4] for b in blocks if b[4].strip()])
        
        # –û—á–∏—â–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã OCR –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.settings["clean_ocr_artifacts"]:
            text = self._clean_ocr_artifacts(text)
        
        return text
    
    def _extract_page_images(self, page, page_num) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        images = []
        image_list = page.get_images(full=True)
        
        for img_index, img in enumerate(image_list):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            xref = img[0]
            pix = fitz.Pixmap(page.parent, xref)
            
            if pix.n - pix.alpha < 4:  # GRAY –∏–ª–∏ RGB
                img_data = {
                    "page": page_num,
                    "index": img_index,
                    "width": pix.width,
                    "height": pix.height,
                    "placeholder": f"[IMAGE_P{page_num:03d}_I{img_index:02d}]"
                }
                images.append(img_data)
            
            pix = None
        
        return images
    
    def _determine_chapter_structure(self, doc, all_pages) -> List[Dict]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥–ª–∞–≤ –¥–ª—è PDF"""
        chapters = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        toc = doc.get_toc()
        
        if toc and len(toc) > 0:
            print(f"üìë –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ ({len(toc)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
            chapters = self._extract_chapters_by_toc(doc, all_pages, toc)
        else:
            # –ò—â–µ–º –≥–ª–∞–≤—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            print("üîç –ò—â–µ–º –≥–ª–∞–≤—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º...")
            chapters = self._detect_chapters_by_patterns(all_pages)
            
            if not chapters or len(chapters) < 2:
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
                print(f"üìä –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ {self.settings['pages_per_chapter']} —Å—Ç—Ä–∞–Ω–∏—Ü")
                chapters = self._split_by_pages(all_pages)
        
        return chapters
    
    def _extract_chapters_by_toc(self, doc, all_pages, toc) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–∞–≤ –ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é"""
        chapters = []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º TOC - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤—ã –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
        main_chapters = []
        for item in toc:
            level, title, page_num = item[0], item[1], item[2] - 1
            if level <= 2 and page_num >= 0:  # –£—Ä–æ–≤–µ–Ω—å 1 –∏–ª–∏ 2
                main_chapters.append({
                    'title': self._clean_chapter_title(title),
                    'start_page': page_num,
                    'level': level
                })
        
        if not main_chapters:
            return []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –≥–ª–∞–≤
        for i, chapter in enumerate(main_chapters):
            if i + 1 < len(main_chapters):
                end_page = main_chapters[i + 1]['start_page']
            else:
                end_page = len(all_pages)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –≥–ª–∞–≤—ã
            chapter_pages = all_pages[chapter['start_page']:end_page]
            chapter_text = "\n".join([p['text'] for p in chapter_pages])
            
            # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            chapter_images = []
            for p in chapter_pages:
                chapter_images.extend(p.get('images', []))
            
            chapters.append({
                'title': chapter['title'],
                'text': chapter_text,
                'start_page': chapter['start_page'],
                'end_page': end_page,
                'pages': chapter_pages,
                'images': chapter_images
            })
        
        return chapters
    
    def _detect_chapters_by_patterns(self, all_pages) -> List[Dict]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–ª–∞–≤ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ —Ç–µ–∫—Å—Ç–µ"""
        chapters = []
        chapter_patterns = [
            r'^(Chapter|CHAPTER|–ì–ª–∞–≤–∞)\s+(\d+|[IVX]+)',
            r'^(Part|PART|–ß–∞—Å—Ç—å)\s+(\d+|[IVX]+)',
            r'^(\d+)\.\s+[A-Z–ê-–Ø][a-z–∞-—èA-Za-z\s]+',
            r'^(Section|SECTION|–†–∞–∑–¥–µ–ª)\s+(\d+|[IVX]+)',
        ]
        
        current_chapter = None
        
        for page_idx, page_data in enumerate(all_pages):
            lines = page_data['text'].split('\n')[:10]  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
            
            for line in lines:
                line = line.strip()
                if len(line) < 3:
                    continue
                    
                for pattern in chapter_patterns:
                    if re.match(pattern, line):
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –≥–ª–∞–≤—É
                        if current_chapter:
                            chapters.append(current_chapter)
                        
                        # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –≥–ª–∞–≤—É
                        current_chapter = {
                            'title': self._clean_chapter_title(line),
                            'text': page_data['text'],
                            'start_page': page_idx,
                            'end_page': page_idx + 1,
                            'pages': [page_data],
                            'images': page_data.get('images', [])
                        }
                        break
                else:
                    continue
                break
            else:
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â—É—é –≥–ª–∞–≤—É
                if current_chapter:
                    current_chapter['text'] += "\n" + page_data['text']
                    current_chapter['end_page'] = page_idx + 1
                    current_chapter['pages'].append(page_data)
                    current_chapter['images'].extend(page_data.get('images', []))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–ª–∞–≤—É
        if current_chapter:
            chapters.append(current_chapter)
        
        return chapters
    
    def _split_by_pages(self, all_pages) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –≥–ª–∞–≤—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–∞–Ω–∏—Ü"""
        chapters = []
        pages_per_chapter = self.settings['pages_per_chapter']
        
        for i in range(0, len(all_pages), pages_per_chapter):
            chapter_pages = all_pages[i:i + pages_per_chapter]
            chapter_text = "\n".join([p['text'] for p in chapter_pages])
            
            # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            chapter_images = []
            for p in chapter_pages:
                chapter_images.extend(p.get('images', []))
            
            chapter_num = len(chapters) + 1
            chapters.append({
                'title': f"–ß–∞—Å—Ç—å {chapter_num}",
                'text': chapter_text,
                'start_page': i,
                'end_page': min(i + pages_per_chapter, len(all_pages)),
                'pages': chapter_pages,
                'images': chapter_images
            })
        
        return chapters
    
    def _analyze_text_structure(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞"""
        stats = {
            'char_count': len(text),
            'word_count': len(text.split()),
            'newline_count': text.count('\n'),
            'double_newline_count': text.count('\n\n'),
            'sentence_count': 0,
            'paragraph_count': 0,
            'has_chapters': False,
            'avg_paragraph_length': 0
        }
        
        # –°—á–∏—Ç–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(text)
        stats['sentence_count'] = len(sentences)
        
        # –°—á–∏—Ç–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if stats['double_newline_count'] > 10:
            # –ï—Å—Ç—å —è–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            paragraphs = text.split('\n\n')
            stats['paragraph_count'] = len([p for p in paragraphs if len(p.strip()) > 20])
        elif stats['newline_count'] > stats['sentence_count'] * 0.5:
            # –ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
            paragraphs = text.split('\n')
            stats['paragraph_count'] = len([p for p in paragraphs if len(p.strip()) > 20])
        else:
            # –ù–µ—Ç —è–≤–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            stats['paragraph_count'] = 0
        
        if stats['paragraph_count'] > 0:
            stats['avg_paragraph_length'] = stats['char_count'] / stats['paragraph_count']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≥–ª–∞–≤
        chapter_markers = ['Chapter', 'CHAPTER', 'Part', 'PART', '–ì–ª–∞–≤–∞', '–ß–∞—Å—Ç—å']
        for marker in chapter_markers:
            if marker in text:
                stats['has_chapters'] = True
                break
        
        return stats
    
    def _detect_text_chapters(self, full_text: str, text_stats: Dict) -> List[Dict]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–ª–∞–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–∞–π–ª–µ"""
        chapters = []
        
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ —è–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –≥–ª–∞–≤
        if text_stats['has_chapters']:
            chapters = self._find_chapter_markers(full_text)
        
        # –ú–µ—Ç–æ–¥ 2: –†–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É —Å —É–º–Ω—ã–º–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏
        if not chapters:
            print("üìä –ì–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É...")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, —Å–æ–∑–¥–∞–µ–º –∏—Ö –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if text_stats['paragraph_count'] == 0:
                print(f"üîÑ –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ {text_stats['sentence_count']} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...")
                paragraphs = self._create_smart_paragraphs(full_text)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                if text_stats['double_newline_count'] > 10:
                    paragraphs = full_text.split('\n\n')
                else:
                    paragraphs = full_text.split('\n')
                
                paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 20]
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –≥–ª–∞–≤—ã
            chapters = self._group_paragraphs_into_chapters(paragraphs)
        
        return chapters
    
    def _find_chapter_markers(self, text: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≥–ª–∞–≤ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º"""
        chapters = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–ª–∞–≤
        patterns = [
            r'(Chapter|CHAPTER)\s+(\d+|[IVX]+)[^\n]*',
            r'(Part|PART)\s+(\d+|[IVX]+)[^\n]*',
            r'(–ì–ª–∞–≤–∞|–ì–õ–ê–í–ê)\s+(\d+|[IVX]+)[^\n]*',
            r'(–ß–∞—Å—Ç—å|–ß–ê–°–¢–¨)\s+(\d+|[IVX]+)[^\n]*',
            r'^\d+\.\s+[A-Z–ê-–Ø][^\n]+',
        ]
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        combined_pattern = '|'.join(patterns)
        
        # –ò—â–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matches = list(re.finditer(combined_pattern, text, re.MULTILINE))
        
        if not matches:
            return []
        
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(matches)} –≥–ª–∞–≤")
        
        for i, match in enumerate(matches):
            start_pos = match.start()
            title = self._clean_chapter_title(match.group(0))
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü –≥–ª–∞–≤—ã
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(text)
            
            chapter_text = text[start_pos:end_pos]
            
            chapters.append({
                'title': title,
                'text': chapter_text,
                'start_pos': start_pos,
                'end_pos': end_pos
            })
        
        return chapters
    
    def _create_smart_paragraphs(self, text: str) -> List[str]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –∏–∑ –Ω–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(text)
        
        paragraphs = []
        current_para = []
        sentence_count = 0
        
        for i, sentence in enumerate(sentences):
            current_para.append(sentence)
            sentence_count += 1
            
            # –£—Å–ª–æ–≤–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            should_break = False
            
            # 1. –î–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if sentence_count >= self.settings['sentences_per_paragraph']:
                should_break = True
            
            # 2. –ö–æ–Ω–µ—Ü –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –±–ª–æ–∫–∞
            elif self._is_logical_break(sentence):
                should_break = True
            
            # 3. –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –±–ª–æ–∫–∞
            elif i + 1 < len(sentences) and self._is_new_section(sentences[i + 1]):
                should_break = True
            
            if should_break:
                if current_para:
                    para_text = ' '.join(current_para).strip()
                    if para_text:
                        paragraphs.append(para_text)
                current_para = []
                sentence_count = 0
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        if current_para:
            para_text = ' '.join(current_para).strip()
            if para_text:
                paragraphs.append(para_text)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
        return paragraphs
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –ó–∞—â–∏—â–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        protected = text
        
        # –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
        abbrevs = [
            'Mr', 'Mrs', 'Dr', 'Ms', 'Prof', 'Sr', 'Jr',
            'Inc', 'Ltd', 'Corp', 'Co', 'vs', 'etc', 'i.e', 'e.g',
            'Ph.D', 'M.D', 'B.A', 'M.A', 'B.S', 'M.S'
        ]
        
        for abbr in abbrevs:
            protected = re.sub(f'\\b{re.escape(abbr)}\\.', f'{abbr}<DOT>', protected, flags=re.IGNORECASE)
        
        # –ó–∞—â–∏—â–∞–µ–º —á–∏—Å–ª–∞ —Å —Ç–æ—á–∫–∞–º–∏
        protected = re.sub(r'(\d+)\.(\d+)', r'\1<DOT>\2', protected)
        
        # –ó–∞—â–∏—â–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã
        protected = re.sub(r'\b([A-Z])\.', r'\1<DOT>', protected)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', protected)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏
        sentences = [s.replace('<DOT>', '.') for s in sentences]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –ø—É—Å—Ç—ã–µ
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        return sentences
    
    def _is_logical_break(self, sentence: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω–µ—Ü —Ä–∞–∑–¥–µ–ª–∞"""
        end_markers = [
            'In conclusion', 'To summarize', 'In summary', 'Finally',
            'Therefore', 'Thus', 'Hence', 'As a result',
            '–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–ò—Ç–∞–∫', '–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ'
        ]
        
        sentence_lower = sentence.lower()
        for marker in end_markers:
            if marker.lower() in sentence_lower[:50]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–æ–Ω–µ—Ü –≥–ª–∞–≤—ã/—Ä–∞–∑–¥–µ–ª–∞
        if re.search(r'(Chapter|Part|Section)\s+\d+', sentence):
            return True
        
        return False
    
    def _is_new_section(self, sentence: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞"""
        start_markers = [
            'Chapter', 'Part', 'Section', 'Introduction',
            '–ì–ª–∞–≤–∞', '–ß–∞—Å—Ç—å', '–†–∞–∑–¥–µ–ª', '–í–≤–µ–¥–µ–Ω–∏–µ',
            'First', 'Second', 'Third', 'Next', 'Another',
            '–ü–µ—Ä–≤—ã–π', '–í—Ç–æ—Ä–æ–π', '–¢—Ä–µ—Ç–∏–π', '–î–∞–ª–µ–µ'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        first_words = sentence.split()[:3]
        for word in first_words:
            if word.strip('.,!?:;') in start_markers:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        if re.match(r'^\d+[\.)]\s+', sentence):
            return True
        
        return False
    
    def _group_paragraphs_into_chapters(self, paragraphs: List[str]) -> List[Dict]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ –≥–ª–∞–≤—ã"""
        chapters = []
        chars_per_chapter = self.settings['chars_per_chapter']
        
        current_chapter = []
        current_size = 0
        chapter_num = 1
        
        for para in paragraphs:
            para_size = len(para)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ—Ä–∞ –ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –≥–ª–∞–≤—É
            if current_size + para_size > chars_per_chapter and current_chapter:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –≥–ª–∞–≤—É
                chapter_text = '\n\n'.join(current_chapter)
                chapters.append({
                    'title': f"–ß–∞—Å—Ç—å {chapter_num}",
                    'text': chapter_text,
                    'paragraphs': current_chapter.copy()
                })
                
                current_chapter = [para]
                current_size = para_size
                chapter_num += 1
            else:
                current_chapter.append(para)
                current_size += para_size
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–ª–∞–≤—É
        if current_chapter:
            chapter_text = '\n\n'.join(current_chapter)
            chapters.append({
                'title': f"–ß–∞—Å—Ç—å {chapter_num}",
                'text': chapter_text,
                'paragraphs': current_chapter
            })
        
        return chapters
    
    def _clean_chapter_title(self, title: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≥–ª–∞–≤—ã"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        title = re.sub(r'[=\-_*#]{3,}', '', title)
        title = re.sub(r'^\s*[\d.]+\s*', '', title)  # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –≤ –Ω–∞—á–∞–ª–µ
        title = ' '.join(title.split())  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(title) > 100:
            title = title[:97] + "..."
        
        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—É—Å—Ç–æ–π, –¥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π
        if not title.strip():
            title = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        
        return title
    
    def _clean_ocr_artifacts(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ OCR"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏–≥–∞—Ç—É—Ä—ã
        ligatures = {'Ô¨Å': 'fi', 'Ô¨Ç': 'fl', 'Ô¨Ä': 'ff', 'Ô¨É': 'ffi', 'Ô¨Ñ': 'ffl'}
        for lig, replacement in ligatures.items():
            text = text.replace(lig, replacement)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º Unicode
        text = unicodedata.normalize('NFKC', text)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r' +', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        text = re.sub(r'(\w)-\s+(\w)', r'\1\2', text)
        
        return text
    
    def _save_chapters(self, chapters_data: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–∞–≤ –≤ —Ñ–∞–π–ª—ã"""
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(chapters_data)} –≥–ª–∞–≤...")
        
        for chapter_num, chapter_data in enumerate(tqdm(chapters_data, desc="–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–∞–≤")):
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            if 'paragraphs' in chapter_data:
                # –£–∂–µ –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                paragraphs = chapter_data['paragraphs']
            else:
                # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
                text = chapter_data.get('text', '')
                if self.settings['smart_paragraph_split']:
                    paragraphs = self._smart_split_into_paragraphs(text)
                else:
                    paragraphs = text.split('\n\n')
                    paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > self.settings['min_paragraph_length']]
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if 'images' in chapter_data and chapter_data['images']:
                paragraphs = self._insert_image_placeholders(paragraphs, chapter_data['images'])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—É
            self._save_single_chapter(
                chapter_data['title'],
                paragraphs,
                chapter_num,
                chapter_data
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.metadata["chapters"].append({
                "number": chapter_num,
                "title": chapter_data['title'],
                "start_page": chapter_data.get('start_page', 0),
                "end_page": chapter_data.get('end_page', 0),
                "paragraph_count": len(paragraphs),
                "word_count": sum(len(p.split()) for p in paragraphs),
                "char_count": sum(len(p) for p in paragraphs),
                "has_images": len(chapter_data.get('images', [])) > 0,
                "status": "extracted"
            })
    
    def _smart_split_into_paragraphs(self, text: str) -> List[str]:
        """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if '\n\n' in text and text.count('\n\n') > 5:
            paragraphs = text.split('\n\n')
            return [p.strip() for p in paragraphs if len(p.strip()) > self.settings['min_paragraph_length']]
        
        # –ï—Å–ª–∏ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
        if '\n' in text and text.count('\n') > text.count('. ') * 0.8:
            paragraphs = []
            current_para = []
            
            for line in text.split('\n'):
                line = line.strip()
                if not line:
                    if current_para:
                        paragraphs.append(' '.join(current_para))
                        current_para = []
                else:
                    current_para.append(line)
            
            if current_para:
                paragraphs.append(' '.join(current_para))
            
            return paragraphs
        
        # –ò–Ω–∞—á–µ —Å–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        return self._create_smart_paragraphs(text)
    
    def _insert_image_placeholders(self, paragraphs: List[str], images: List[Dict]) -> List[str]:
        """–í—Å—Ç–∞–≤–∫–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –Ω—É–∂–Ω—ã–µ –º–µ—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞: –≤—Å—Ç–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ
        if not images:
            return paragraphs
        
        result = []
        images_per_para = max(1, len(paragraphs) // len(images))
        image_idx = 0
        
        for i, para in enumerate(paragraphs):
            result.append(para)
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ N-–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            if (i + 1) % images_per_para == 0 and image_idx < len(images):
                result.append(images[image_idx]['placeholder'])
                image_idx += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∫–æ–Ω–µ—Ü
        while image_idx < len(images):
            result.append(images[image_idx]['placeholder'])
            image_idx += 1
        
        return result
    
    def _save_single_chapter(self, title: str, paragraphs: List[str], chapter_num: int, extra_data: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–¥–Ω–æ–π –≥–ª–∞–≤—ã –≤ JSON"""
        filename = f"chapter_{chapter_num:03d}.json"
        filepath = self.output_dir / filename
        
        chapter_data = {
            "number": chapter_num,
            "title": title,
            "paragraphs": paragraphs,
            "paragraph_count": len(paragraphs),
            "word_count": sum(len(p.split()) for p in paragraphs),
            "char_count": sum(len(p) for p in paragraphs),
            "source_format": self.file_format,
            "extraction_method": self.metadata["extraction_method"]
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'start_page' in extra_data:
            chapter_data['start_page'] = extra_data['start_page']
        if 'end_page' in extra_data:
            chapter_data['end_page'] = extra_data['end_page']
        if 'images' in extra_data and extra_data['images']:
            chapter_data['image_count'] = len(extra_data['images'])
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(chapter_data, f, ensure_ascii=False, indent=2)
    
    def _finalize_extraction(self):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        self.metadata["extraction_complete"] = True
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_words = sum(ch['word_count'] for ch in self.metadata['chapters'])
        total_chars = sum(ch['char_count'] for ch in self.metadata['chapters'])
        total_paragraphs = sum(ch['paragraph_count'] for ch in self.metadata['chapters'])
        
        self.metadata['statistics'] = {
            'total_chapters': len(self.metadata['chapters']),
            'total_words': total_words,
            'total_chars': total_chars,
            'total_paragraphs': total_paragraphs,
            'avg_chapter_words': total_words // max(1, len(self.metadata['chapters'])),
            'avg_paragraph_words': total_words // max(1, total_paragraphs)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self._save_metadata()
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏
        print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –ì–ª–∞–≤: {self.metadata['statistics']['total_chapters']}")
        print(f"   ‚Ä¢ –ü–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {self.metadata['statistics']['total_paragraphs']}")
        print(f"   ‚Ä¢ –°–ª–æ–≤: {self.metadata['statistics']['total_words']:,}")
        print(f"   ‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {self.metadata['statistics']['total_chars']:,}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –¥–ª—è PDF
        if self.file_format == 'pdf' and self.metadata.get('total_pages', 0) > 0:
            self._verify_page_coverage()
    
    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        metadata_file = self.output_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
    
    def _verify_page_coverage(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è PDF"""
        if self.file_format != 'pdf':
            return
        
        covered_pages = set()
        for chapter in self.metadata['chapters']:
            for page in range(chapter.get('start_page', 0), chapter.get('end_page', 0)):
                covered_pages.add(page)
        
        total_pages = self.metadata.get('total_pages', 0)
        all_pages = set(range(total_pages))
        missing_pages = all_pages - covered_pages
        
        if missing_pages:
            print(f"‚ö†Ô∏è  –ù–µ –ø–æ–∫—Ä—ã—Ç—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {sorted(missing_pages)[:20]}...")
            if len(missing_pages) > 20:
                print(f"    (–∏ –µ—â–µ {len(missing_pages) - 20} —Å—Ç—Ä–∞–Ω–∏—Ü)")
        else:
            print("‚úÖ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF –ø–æ–∫—Ä—ã—Ç—ã")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∫–Ω–∏–≥')
    parser.add_argument('input', nargs='?', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–Ω–∏–≥–∏')
    parser.add_argument('--output-dir', '-o', default='extracted_fixed', 
                      help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: extracted_fixed)')
    parser.add_argument('--sentences-per-para', '-s', type=int, default=8,
                      help='–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ –¥–ª—è –Ω–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8)')
    parser.add_argument('--chars-per-chapter', '-c', type=int, default=10000,
                      help='–°–∏–º–≤–æ–ª–æ–≤ –≤ –≥–ª–∞–≤–µ –ø—Ä–∏ —Ä–∞–∑–±–∏–≤–∫–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10000)')
    parser.add_argument('--pages-per-chapter', '-p', type=int, default=30,
                      help='–°—Ç—Ä–∞–Ω–∏—Ü –≤ –≥–ª–∞–≤–µ –¥–ª—è PDF –±–µ–∑ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30)')
    parser.add_argument('--no-images', action='store_true',
                      help='–ù–µ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PDF')
    parser.add_argument('--no-ocr-clean', action='store_true',
                      help='–ù–µ –æ—á–∏—â–∞—Ç—å OCR –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã')
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    if args.input:
        input_file = args.input
    else:
        # –ò—â–µ–º book.pdf –∏–ª–∏ book.txt
        if os.path.exists("book.pdf"):
            input_file = "book.pdf"
        elif os.path.exists("book.txt"):
            input_file = "book.txt"
        else:
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            print("   –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ book.pdf/book.txt –≤ —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é")
            return
    
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª {input_file} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        return
    
    print("="*60)
    print("üìö –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –≠–ö–°–¢–†–ê–ö–¢–û–† –ö–ù–ò–ì v2.0")
    print("="*60)
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    extractor = UniversalBookExtractor(input_file, args.output_dir)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if args.sentences_per_para:
        extractor.settings['sentences_per_paragraph'] = args.sentences_per_para
    if args.chars_per_chapter:
        extractor.settings['chars_per_chapter'] = args.chars_per_chapter
    if args.pages_per_chapter:
        extractor.settings['pages_per_chapter'] = args.pages_per_chapter
    if args.no_images:
        extractor.settings['extract_images'] = False
    if args.no_ocr_clean:
        extractor.settings['clean_ocr_artifacts'] = False
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
    metadata = extractor.extract_all()
    
    if metadata:
        print("\n" + "="*60)
        print("üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø:")
        print("="*60)
        
        for chapter in metadata['chapters'][:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 –≥–ª–∞–≤
            print(f"  –ì–ª–∞–≤–∞ {chapter['number']:03d}: {chapter['title'][:50]:50s} "
                  f"({chapter['paragraph_count']:3d} –ø–∞—Ä., {chapter['word_count']:5d} —Å–ª–æ–≤)")
        
        if len(metadata['chapters']) > 10:
            print(f"  ... –∏ –µ—â–µ {len(metadata['chapters']) - 10} –≥–ª–∞–≤")
        
        print("\n‚úÖ –ì–æ—Ç–æ–≤–æ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        print("   ‚Ä¢ –ü–µ—Ä–µ–≤–æ–¥: python3 03_translate_parallel.py --all")
        print("   ‚Ä¢ –§–∏–≥—É—Ä—ã:  python3 02_extract_figures.py")
        print("   ‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: python3 09_extract_book_context.py")


if __name__ == "__main__":
    main()