#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ —á–µ—Ä–µ–∑ DeepSeek API
"""

import json
import os
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env –µ—Å–ª–∏ –µ—Å—Ç—å
env_file = Path('.env')
if env_file.exists():
    with open(env_file) as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                if value.startswith('"') and value.endswith('"'):
                    value = value[1:-1]
                elif value.startswith("'") and value.endswith("'"):
                    value = value[1:-1]
                os.environ[key] = value

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ deepseek_translator
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from deepseek_translator import DeepSeekTranslator


class PhoneticGenerator:
    def __init__(self, api_key: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π"""
        self.translator = DeepSeekTranslator(api_key)
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        self.examples = {
            # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            'CMMI': '—Å–∏-—ç–º-—ç–º-–∞–π',
            'SEI': '—ç—Å-–∏-–∞–π',
            'API': '—ç–π-–ø–∏-–∞–π',
            'URL': '—é-–∞—Ä-—ç–ª',
            'HTML': '—ç–π—á-—Ç–∏-—ç–º-—ç–ª',
            
            # –°–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            'Process Area': '–ø—Ä–æÃÅ—Ü–µ—Å—Å —çÃÅ—Ä–∏–∞',
            'Maturity Level': '–º—ç—Ç—å—éÃÅ—Ä–∏—Ç–∏ –ª–µÃÅ–≤–µ–ª',
            'Software Engineering': '—Å–æÃÅ—Ñ—Ç–≤–µ—Ä —ç–Ω–∂–∏–Ω–∏ÃÅ—Ä–∏–Ω–≥',
            
            # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            'Development': '–¥–µ–≤–µ–ª–æÃÅ–ø–º–µ–Ω—Ç',
            'Services': '—Å—ë—Ä–≤–∏—Å–µ–∑',
            'Agile': '—çÃÅ–¥–∂–∞–π–ª',
            'Level': '–ª–µ–≤–µ–ª',
            
            # –° —Ç–æ—á–∫–∞–º–∏
            'C.M.M.I.': '—Å–∏-—ç–º-—ç–º-–∞–π',
            'U.S.A.': '—é-—ç—Å-—ç–π'
        }
        
    def create_phonetic_prompt(self, terms: List[str]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π"""
        examples_text = "\n".join([
            f"{en}: {ru}" 
            for en, ru in list(self.examples.items())[:10]
        ])
        
        terms_text = "\n".join(terms)
        
        prompt = f"""–°–æ–∑–¥–∞–π —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤.
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –ø–æ–º–æ—á—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—é –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –≤—Å–ª—É—Ö —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–æ–º —Ä–µ—á–∏.

–ü—Ä–∞–≤–∏–ª–∞:
1. –î–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä - –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç—å –ø–æ –±—É–∫–≤–∞–º —á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å (FBI ‚Üí —ç—Ñ-–±–∏-–∞–π)
2. –î–ª—è —Å–ª–æ–≤ - —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å—å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π —Å —É–¥–∞—Ä–µ–Ω–∏—è–º–∏ (Development ‚Üí –¥–µ–≤–µ–ª–æÃÅ–ø–º–µ–Ω—Ç)
3. –î–ª—è —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ - —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (Process Area ‚Üí –ø—Ä–æÃÅ—Ü–µ—Å—Å —çÃÅ—Ä–∏–∞)
4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–¥–∞—Ä–µ–Ω–∏—è –≥–¥–µ –Ω—É–∂–Ω–æ (–∑–Ω–∞–∫ ÃÅ –ø–æ—Å–ª–µ —É–¥–∞—Ä–Ω–æ–π –≥–ª–∞—Å–Ω–æ–π)

–ü—Ä–∏–º–µ—Ä—ã:
{examples_text}

–¢–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏:
{terms_text}

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - JSON –æ–±—ä–µ–∫—Ç, –≥–¥–µ –∫–ª—é—á - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ä–º–∏–Ω, –∑–Ω–∞—á–µ–Ω–∏–µ - —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:
{{"—Ç–µ—Ä–º–∏–Ω": "—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è"}}"""
        
        return prompt
    
    def generate_batch(self, terms: List[str], batch_size: int = 20) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –ø–∞—á–∫–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        if not terms:
            return {}
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        terms_batch = terms[:batch_size]
        
        prompt = self.create_phonetic_prompt(terms_batch)
        
        try:
            response = self.translator.translate_text(
                prompt,
                system_message="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–æ–Ω–µ—Ç–∏–∫–µ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –°–æ–∑–¥–∞–µ—à—å —Ç–æ—á–Ω—ã–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–æ–≤ —Ä–µ—á–∏.",
                temperature=0.3,
                max_tokens=2000
            )
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            try:
                # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_text = response[json_start:json_end]
                    return json.loads(json_text)
                else:
                    print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω JSON –≤ –æ—Ç–≤–µ—Ç–µ")
                    return {}
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≤—Ä—É—á–Ω—É—é
                return self.parse_manual_response(response, terms_batch)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return {}
    
    def parse_manual_response(self, response: str, terms: List[str]) -> Dict[str, str]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –≤—Ä—É—á–Ω—É—é, –µ—Å–ª–∏ JSON –Ω–µ —É–¥–∞–ª—Å—è"""
        result = {}
        lines = response.split('\n')
        
        for line in lines:
            for term in terms:
                if term in line:
                    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "term: transcription" –∏–ª–∏ "term ‚Üí transcription"
                    for sep in [':', '‚Üí', '-', '‚Äî']:
                        if sep in line:
                            parts = line.split(sep, 1)
                            if len(parts) == 2:
                                key = parts[0].strip().strip('"\'')
                                value = parts[1].strip().strip('"\'')
                                if term in key:
                                    result[term] = value
                                    break
        
        return result
    
    def generate_all(self, terms_file: str = "extracted_terms.json", 
                    batch_size: int = 20, workers: int = 5):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã
        with open(terms_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        all_terms = set()
        for term_type, terms_list in data.get('terms_by_type', {}).items():
            all_terms.update(terms_list)
        
        # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ –µ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        terms_to_process = [t for t in all_terms if t not in self.examples]
        
        print(f"üìù –í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(all_terms)}")
        print(f"üîÑ –ù—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(terms_to_process)}")
        
        if not terms_to_process:
            print("‚úÖ –í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã —É–∂–µ –∏–º–µ—é—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
            return self.examples
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
        batches = []
        for i in range(0, len(terms_to_process), batch_size):
            batch = terms_to_process[i:i+batch_size]
            batches.append(batch)
        
        print(f"üì¶ –ë–∞—Ç—á–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(batches)}")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_phonetics = dict(self.examples)  # –ù–∞—á–∏–Ω–∞–µ–º —Å –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(self.generate_batch, batch): batch 
                for batch in batches
            }
            
            with tqdm(total=len(batches), desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π") as pbar:
                for future in as_completed(futures):
                    batch = futures[future]
                    try:
                        result = future.result()
                        all_phonetics.update(result)
                        pbar.update(1)
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
                        if result:
                            example = list(result.items())[0]
                            print(f"  ‚úì {example[0]} ‚Üí {example[1]}")
                            
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {e}")
                        pbar.update(1)
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(0.5)
        
        return all_phonetics
    
    def save_phonetics(self, phonetics: Dict[str, str], output_file: str = "phonetics.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        sorted_phonetics = dict(sorted(phonetics.items()))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        output = {
            "version": "1.0",
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_terms": len(phonetics),
            "phonetics": sorted_phonetics,
            "statistics": {
                "abbreviations": len([k for k in phonetics.keys() if k.isupper()]),
                "compound_terms": len([k for k in phonetics.keys() if ' ' in k]),
                "single_words": len([k for k in phonetics.keys() if ' ' not in k and not k.isupper()])
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_file}")
        return output


def main():
    parser = argparse.ArgumentParser(description='–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —á–µ—Ä–µ–∑ DeepSeek')
    parser.add_argument('--terms', default='extracted_terms.json',
                       help='–§–∞–π–ª —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏')
    parser.add_argument('--output', default='phonetics.json',
                       help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π')
    parser.add_argument('--batch-size', type=int, default=20,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--workers', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤')
    parser.add_argument('--api-key', help='DeepSeek API –∫–ª—é—á')
    
    args = parser.parse_args()
    
    print("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —á–µ—Ä–µ–∑ DeepSeek")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏
    if not Path(args.terms).exists():
        print(f"‚ùå –§–∞–π–ª {args.terms} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python3 07_extract_terms.py")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
    api_key = args.api_key or os.environ.get('DEEPSEEK_API_KEY')
    if not api_key:
        print("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ DEEPSEEK_API_KEY –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --api-key")
        return
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = PhoneticGenerator(api_key)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    phonetics = generator.generate_all(
        args.terms,
        batch_size=args.batch_size,
        workers=args.workers
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_data = generator.save_phonetics(phonetics, args.output)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {output_data['total_terms']}")
    print(f"  ‚Ä¢ –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä: {output_data['statistics']['abbreviations']}")
    print(f"  ‚Ä¢ –°–æ—Å—Ç–∞–≤–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {output_data['statistics']['compound_terms']}")
    print(f"  ‚Ä¢ –û—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {output_data['statistics']['single_words']}")
    
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"\nüí° –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∞—É–¥–∏–æ–∫–Ω–∏–≥–µ:")
    print(f"   python3 05_create_audiobook.py --phonetics {args.output}")


if __name__ == "__main__":
    main()