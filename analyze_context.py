#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–µ—Ä–µ–≤–æ–¥–∞
"""

import json
import re
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import Counter
from deepseek_translator import DeepSeekTranslator

class ContextAnalyzer:
    def __init__(self):
        self.translator = DeepSeekTranslator()
        self.context_file = Path('translation_context.yaml')
        self.analysis_cache = Path('context/analysis_cache.json')
        self.analysis_cache.parent.mkdir(exist_ok=True)
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ç–µ–∫—Å—Ç–∞
        self.technical_markers = {
            'it': ['software', 'code', 'algorithm', 'database', 'API', 'framework', 
                   'programming', 'development', 'deploy', 'server', 'backend', 'frontend'],
            'engineering': ['design', 'specification', 'requirement', 'system', 'architecture',
                           'component', 'module', 'interface', 'implementation'],
            'management': ['employee', 'worker', 'manager', 'organization', 'workplace',
                          'motivation', 'satisfaction', 'engagement', 'productivity',
                          'leadership', 'team', 'performance', 'job', 'career'],
            'psychology': ['behavior', 'motivation', 'satisfaction', 'engagement', 'meaning',
                          'psychology', 'attitude', 'human nature', 'perception'],
            'business': ['company', 'CEO', 'profit', 'market', 'customer', 'strategy',
                        'revenue', 'business', 'corporation', 'enterprise', 'industry'],
            'science': ['research', 'hypothesis', 'experiment', 'analysis', 'data', 
                       'methodology', 'results', 'conclusion', 'theory']
        }
        
        self.style_indicators = {
            'formal': ['therefore', 'furthermore', 'consequently', 'nevertheless', 
                      'whereas', 'hereby', 'pursuant'],
            'informal': ["let's", "you'll", "we'll", "don't", "won't", "can't", 
                        'gonna', 'wanna', 'kinda'],
            'academic': ['abstract', 'methodology', 'literature review', 'hypothesis',
                        'conclusion', 'references', 'citation', 'et al.'],
            'instructional': ['step 1', 'step 2', 'how to', 'tutorial', 'guide',
                             'follow these', 'make sure', 'be careful']
        }
    
    def analyze_text_sample(self, text: str, sample_size: int = 5000) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –µ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        
        # –ë–µ—Ä–µ–º –æ–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞
        sample = text[:sample_size] if len(text) > sample_size else text
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        words = sample.lower().split()
        word_count = len(words)
        sentences = re.split(r'[.!?]+', sample)
        avg_sentence_length = len(words) / max(len(sentences), 1)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–µ–∫—Å—Ç–∞
        text_type = self._detect_text_type(sample)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π
        domain = self._detect_domain(sample, words)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å
        style = self._detect_style(sample, words, avg_sentence_length)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å
        tech_level = self._detect_technical_level(sample, words)
        
        return {
            'text_type': text_type,
            'domain': domain,
            'style': style,
            'technical_level': tech_level,
            'statistics': {
                'word_count': word_count,
                'avg_sentence_length': avg_sentence_length,
                'unique_words': len(set(words)),
                'lexical_diversity': len(set(words)) / word_count if word_count > 0 else 0
            }
        }
    
    def _detect_text_type(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç–µ–∫—Å—Ç–∞"""
        
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–Ω–∏–≥—É –æ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–µ/—Ä–∞–±–æ—Ç–µ
        work_psychology_markers = ['work', 'worker', 'employee', 'job', 'satisfaction', 
                                  'engagement', 'motivation', 'workplace', 'organization']
        work_count = sum(1 for marker in work_psychology_markers if marker in text_lower)
        
        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤ –æ —Ä–∞–±–æ—Ç–µ - —ç—Ç–æ –±–∏–∑–Ω–µ—Å-–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞
        if work_count >= 4:
            return 'business'  # –ë–∏–∑–Ω–µ—Å-–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –æ —Ä–∞–±–æ—Ç–µ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏–∏
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        academic_markers = ['abstract', 'references', 'doi:', 'isbn', 'et al', 'journal']
        if any(marker in text_lower for marker in academic_markers):
            return 'academic'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (Gallup –∏ —Ç.–¥.)
        if all(marker in text_lower for marker in ['research', 'percent', 'data']):
            if work_count > 3:
                return 'business'  # –ë–∏–∑–Ω–µ—Å-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
            return 'academic'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç  
        if 'chapter' in text_lower and work_count < 3:
            if '"' in text or '"' in text:  # –ï—Å—Ç—å –¥–∏–∞–ª–æ–≥–∏
                return 'fiction'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        if any(marker in text_lower for marker in ['whereas', 'pursuant', 'hereby', 'clause']):
            return 'legal'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        if all(marker in text_lower for marker in ['patient', 'hospital', 'medical']):
            if 'custodian' not in text_lower:  # –ò—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏–∏ –æ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞—Ö –±–æ–ª—å–Ω–∏—Ü
                return 'medical'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∏–∑–Ω–µ—Å —Ç–µ–∫—Å—Ç –ø–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –º–∞—Ä–∫–µ—Ä–∞–º
        business_markers = ['CEO', 'company', 'business', 'management', 'employee']
        if sum(1 for marker in business_markers if marker in text) >= 3:
            return 'business'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        tech_words = re.findall(r'\b[A-Z]{2,}\b', text)  # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        if len(tech_words) > 10 and work_count < 3:
            return 'technical'
        
        return 'general'
    
    def _detect_domain(self, text: str, words: List[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π"""
        
        domain_scores = {}
        for domain, markers in self.technical_markers.items():
            score = sum(1 for word in words if word in markers)
            if score > 0:
                domain_scores[domain] = score
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º management –∏ psychology –µ—Å–ª–∏ –æ–±–∞ –≤—ã—Å–æ–∫–∏–µ
        if 'management' in domain_scores and 'psychology' in domain_scores:
            if domain_scores['management'] > 3 or domain_scores['psychology'] > 2:
                return 'management'  # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É
        
        if domain_scores:
            return max(domain_scores, key=domain_scores.get)
        
        return 'general'
    
    def _detect_style(self, text: str, words: List[str], avg_sentence_length: float) -> Dict:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∏–ª—å —Ç–µ–∫—Å—Ç–∞"""
        
        # –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
        formal_score = sum(1 for word in words if word in self.style_indicators['formal'])
        informal_score = sum(1 for word in words if word in self.style_indicators['informal'])
        
        if formal_score > informal_score * 2:
            formality = 'formal'
        elif informal_score > formal_score * 2:
            formality = 'informal'
        else:
            formality = 'neutral'
        
        # –¢–æ–Ω
        if any(word in words for word in self.style_indicators['academic']):
            tone = 'academic'
        elif any(word in words for word in self.style_indicators['instructional']):
            tone = 'instructional'
        elif avg_sentence_length < 15:
            tone = 'conversational'
        else:
            tone = 'neutral'
        
        # –ê—É–¥–∏—Ç–æ—Ä–∏—è
        technical_terms = len([w for w in words if len(w) > 10])
        if technical_terms / len(words) > 0.1:
            audience = 'specialists'
        elif formality == 'informal':
            audience = 'general'
        else:
            audience = 'professionals'
        
        return {
            'formality': formality,
            'tone': tone,
            'audience': audience
        }
    
    def _detect_technical_level(self, text: str, words: List[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —Ç–µ–∫—Å—Ç–∞"""
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        acronyms = len(re.findall(r'\b[A-Z]{2,}\b', text))
        long_words = len([w for w in words if len(w) > 12])
        special_chars = len(re.findall(r'[=<>‚â§‚â•‚àà‚àÄ‚àÉ\[\]{}()]', text))
        
        tech_score = acronyms + long_words/10 + special_chars/5
        
        if tech_score > 50:
            return 'high'
        elif tech_score > 20:
            return 'medium'
        else:
            return 'low'
    
    def analyze_with_ai(self, text_sample: str) -> Dict:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç AI –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏.

–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤):
{text_sample[:2000]}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø —Ç–µ–∫—Å—Ç–∞ (technical/fiction/academic/business/legal/medical/general)
2. –û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π (it/engineering/management/science/humanities/general)
3. –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å (formal/informal/neutral)
4. –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è (specialists/professionals/students/general)
5. –û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞ (–æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º)
6. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Å—Ç–∏–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "text_type": "...",
    "domain": "...",
    "formality": "...",
    "audience": "...",
    "topic": "...",
    "translation_style": "..."
}}"""

        try:
            response = self.translator.translate_text(
                prompt,
                system_message="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–æ–≤ –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ.",
                temperature=0.3
            )
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        return {}
    
    def generate_context_config(self, analysis: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        
        config = {
            'text_type': analysis.get('text_type', 'general'),
            'domain': analysis.get('domain', 'general'),
            'style': analysis.get('style', {
                'formality': 'neutral',
                'audience': 'general',
                'tone': 'neutral'
            }),
            'technical_level': analysis.get('technical_level', 'medium')
        }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        config['system_prompt'] = self._generate_system_prompt(config)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        if config['text_type'] == 'technical':
            config['processing'] = {
                'preserve_terms': True,
                'use_russian_terms': True,
                'preserve_placeholders': True,
                'preserve_formatting': True
            }
        elif config['text_type'] == 'fiction':
            config['processing'] = {
                'preserve_names': True,
                'adapt_idioms': True,
                'literary_style': True
            }
        elif config['text_type'] == 'academic':
            config['processing'] = {
                'strict_terminology': True,
                'preserve_citations': True,
                'formal_style': True
            }
        
        return config
    
    def _generate_system_prompt(self, config: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_parts = ["–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫."]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        if config['domain'] == 'it':
            prompt_parts.append("–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: IT –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è.")
            prompt_parts.append("–ò—Å–ø–æ–ª—å–∑—É–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é —Ä–æ—Å—Å–∏–π—Å–∫—É—é IT-—Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é.")
        elif config['domain'] == 'engineering':
            prompt_parts.append("–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã.")
            prompt_parts.append("–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—É—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é.")
        elif config['domain'] == 'management':
            prompt_parts.append("–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ –∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã.")
            prompt_parts.append("–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–Ω—è—Ç—É—é –≤ –†–æ—Å—Å–∏–∏ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é.")
        elif config['domain'] == 'science':
            prompt_parts.append("–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –Ω–∞—É—á–Ω—ã–µ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã.")
            prompt_parts.append("–°–æ–±–ª—é–¥–∞–π –Ω–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è.")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∏–ª—å
        style = config.get('style', {})
        if style.get('formality') == 'formal':
            prompt_parts.append("–°—Ç–∏–ª—å: —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π.")
        elif style.get('formality') == 'informal':
            prompt_parts.append("–°—Ç–∏–ª—å: –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π.")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—É–¥–∏—Ç–æ—Ä–∏—é
        if style.get('audience') == 'specialists':
            prompt_parts.append("–ê—É–¥–∏—Ç–æ—Ä–∏—è: —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –∏ —ç–∫—Å–ø–µ—Ä—Ç—ã –≤ –¥–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏.")
        elif style.get('audience') == 'students':
            prompt_parts.append("–ê—É–¥–∏—Ç–æ—Ä–∏—è: —Å—Ç—É–¥–µ–Ω—Ç—ã –∏ –Ω–∞—á–∏–Ω–∞—é—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã.")
        elif style.get('audience') == 'general':
            prompt_parts.append("–ê—É–¥–∏—Ç–æ—Ä–∏—è: —à–∏—Ä–æ–∫–∏–π –∫—Ä—É–≥ —á–∏—Ç–∞—Ç–µ–ª–µ–π.")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        prompt_parts.extend([
            "–°–æ—Ö—Ä–∞–Ω—è–π –≤—Å–µ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤–∏–¥–∞ [IMAGE_XXX], [TABLE_XXX] –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.",
            "–°–æ—Ö—Ä–∞–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞.",
            "–ü–µ—Ä–µ–≤–æ–¥–∏ —Ç–æ—á–Ω–æ, –Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞."
        ])
        
        return " ".join(prompt_parts)
    
    def analyze_book(self, book_path: str = None) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å—é –∫–Ω–∏–≥—É –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        
        print("üìö –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏...")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        if book_path is None:
            possible_paths = [
                'extracted_content.json',
                'chapters/chapter_000.json', 
                'book.txt'
            ]
            for path in possible_paths:
                if Path(path).exists():
                    book_path = path
                    break
        
        if book_path is None or not Path(book_path).exists():
            # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –≥–ª–∞–≤, —á–∏—Ç–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é book.txt
            if Path('book.txt').exists():
                with open('book.txt', 'r', encoding='utf-8') as f:
                    full_sample = f.read()[:10000]  # –ü–µ—Ä–≤—ã–µ 10000 —Å–∏–º–≤–æ–ª–æ–≤
            else:
                raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –∫–Ω–∏–≥–∏")
        elif book_path.endswith('.txt'):
            with open(book_path, 'r', encoding='utf-8') as f:
                full_sample = f.read()[:10000]
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON
            with open(book_path, 'r', encoding='utf-8') as f:
                book_data = json.load(f)
            
            # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π
            sample_texts = []
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª –≥–ª–∞–≤—ã
            if 'content' in book_data:
                for para in book_data.get('content', [])[:20]:  # –ü–µ—Ä–≤—ã–µ 20 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                    if isinstance(para, dict) and para.get('type') == 'text':
                        sample_texts.append(para.get('content', ''))
                    elif isinstance(para, str):
                        sample_texts.append(para)
            # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª —Å –≥–ª–∞–≤–∞–º–∏
            elif 'chapters' in book_data:
                for chapter in book_data.get('chapters', [])[:3]:  # –ü–µ—Ä–≤—ã–µ 3 –≥–ª–∞–≤—ã
                    for para in chapter.get('content', [])[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                        if para.get('type') == 'text':
                            sample_texts.append(para.get('content', ''))
            
            full_sample = '\n'.join(sample_texts)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        print("  üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        auto_analysis = self.analyze_text_sample(full_sample)
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º AI –∞–Ω–∞–ª–∏–∑–æ–º –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω API
        ai_analysis = {}
        if self.translator.api_key and self.translator.api_key != 'your_api_key_here':
            print("  ü§ñ AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
            ai_analysis = self.analyze_with_ai(full_sample)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_analysis = {**auto_analysis, **ai_analysis}
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        print("  ‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
        config = self.generate_context_config(final_analysis)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open(self.context_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à –∞–Ω–∞–ª–∏–∑–∞
        with open(self.analysis_cache, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis': final_analysis,
                'config': config,
                'timestamp': str(Path(book_path).stat().st_mtime)
            }, f, ensure_ascii=False, indent=2)
        
        return config
    
    def get_context(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–Ω–æ–≤–æ"""
        
        if self.context_file.exists():
            with open(self.context_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            return self.analyze_book()
    
    def update_translator_context(self, translator: DeepSeekTranslator) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–µ"""
        
        context = self.get_context()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
        if 'system_prompt' in context:
            # Monkey-patch –º–µ—Ç–æ–¥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            original_create_prompt = translator._create_system_prompt
            
            def new_create_prompt(ctx):
                return context['system_prompt']
            
            translator._create_system_prompt = new_create_prompt
        
        print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {context.get('text_type', 'general')} / {context.get('domain', 'general')}")


def main():
    analyzer = ContextAnalyzer()
    
    print("=" * 60)
    print("üìä –ê–ù–ê–õ–ò–ó–ê–¢–û–† –ö–û–ù–¢–ï–ö–°–¢–ê –ü–ï–†–ï–í–û–î–ê")
    print("=" * 60)
    print()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–Ω–∏–≥—É
    config = analyzer.analyze_book()
    
    print()
    print("üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
    print("-" * 40)
    print(f"–¢–∏–ø —Ç–µ–∫—Å—Ç–∞: {config.get('text_type', '–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω')}")
    print(f"–û–±–ª–∞—Å—Ç—å: {config.get('domain', '–æ–±—â–∞—è')}")
    print(f"–°—Ç–∏–ª—å: {config.get('style', {}).get('formality', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π')}")
    print(f"–ê—É–¥–∏—Ç–æ—Ä–∏—è: {config.get('style', {}).get('audience', '–æ–±—â–∞—è')}")
    print(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å: {config.get('technical_level', '—Å—Ä–µ–¥–Ω–∏–π')}")
    print()
    print("üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: translation_context.yaml")
    print("üìù –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞")
    print()
    print("–¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å")
    print("–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞!")


if __name__ == '__main__':
    main()